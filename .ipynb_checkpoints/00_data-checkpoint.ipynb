{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical and repetetive process in every deep learning project is training the neural network model. The basic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training neural networks is usually considered an art or [epistimilogy is some cases](https://www.youtube.com/watch?v=gG5NCkMerHU). Most of the times when working on deep learning applications, its desired that we focus on solving our problem rather than figuring out how to best train the network we come up with. Also, we do not want to write a new training loop for every new project. In that spirit, Let's define `some classes and functions` for getting `Data` in a `usable` format for `training with mini-batch SGD using backpropogation.` This will allow us to add best practices in training a model that can be shared across multiple project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [data-vis course](https://www.kaggle.com/learn/data-visualization-from-non-coder-to-coder)\n",
    "- [Advance NLP with spacy](https://www.datacamp.com/courses/advanced-nlp-with-spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import tensor\n",
    "from torch import optim \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from fastprogress.fastprogress import format_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data, Model and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every machine learning project starts with gathering data that we would like to analyze and build models on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define a function called `get_data` that get's (downloads) our data and converts it into tensor format so that we can utilize our GPU's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_mnist_data(MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'):\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to iterate through our dataset one mini-batch at a time efficiently and in a readable way, We will use `Pytorch's Dataloader` class which is a generator generating our x's and y's on mini-batch at a time. Dataloader expects a Dataset which is a class that allows to index into the dataset to grab a specfic data point. This can be implemented by overriding `__getitem__` function of the class. We will also override `__len__` function to make it easy to get total length of out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i],self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write a convienient function that takes training dataset and validation dataset and return respective dataloaders. We will also `shuffle` our training set every epoch to introduce some randomisation so that network doesn't learn's a specific order in our data. The Dataloader class also take a parameter `batch_size` which is usually determined by memory of the GPU's we have. Because, we never do backprop during validation set, we can have the `double batch_size` during that period as compared to training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=4, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, num_workers=4, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now' lets wrap our dataloaders into a `Databunch` class which just acts as a storage device. We will also make the underlying dataset accesible so that if we want to change it later, we can. Why would we want to do that is an excersise for the reader ;). One will also notice a variable called `c_out`, which is basically the number of classes we are trying to classify. We can automatically detect this from our `labels`, so why not just automate it so that we can decide last layer's number of outputs automatically !."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, c_out=None):\n",
    "        self.train_dl,self.valid_dl,self.c_out = train_dl,valid_dl,c_out\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "        \n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our functions to get the databunch. Along the we will also see what each of the functions and classes defined above takes as input and produces as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use `get_data` function so that we can have our data in tensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check the shapes of things returned by `get_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape , y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have guessed if you are following along, here we are working with MNIST dataset. Why ?? We need a toy dataset to quickly validate our infrastructure. The goal here is to build a flexible dataloading and training loop and not designing architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have `50000 x_train` datapoints each with `y_train`. The x's are flattened images of handwritten digits and the ys are their actual digits in those images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a Dataset for each of our training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our dataloaders, so that we can iterate through our dataset in mini-batches. We will define our batch_size to be equal to 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = y_train.max().item()+1\n",
    "bs=512\n",
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's a classification task, we will be using `categorical_cross_entropy` function as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our goal is little different in this notebook, we will just define a simple one layer neural network with hidden activation size of `50`. We also need a optimizer *(more on this in a later post)*. At a high level, optimizer is the thing resposible for updating out weights and biases while doing stochastic mini-batch gradient descent. Let's wrap the model and the optimizer in a convienient function called `get_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_model(data, nh=50, lr=0.1):\n",
    "    m = data.train_ds.x.shape[1]\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, data.c_out))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A detour: Callbacks in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a utility function that converts a string in `CamelCase` format to a `Snake` format. We will use this functions to register our Callback's name with python in snake format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "\n",
    "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
    "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
    "def camel2snake(name):\n",
    "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
    "    s2 =  re.sub(_camel_re2, r'\\1_\\2', s1)\n",
    "    return s2.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function on a couple of strings so that we get comfortable with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello_world_good_morning_how_are_you'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camel2snake(\"HelloWorldGoodMorningHowAreYou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep_learning_is_amazing'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camel2snake(\"DeepLearningIsAmazing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function which calulates the sum of squares upto a given number. We will also sleep the process for 1 second, so that we can mimic a heavy processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(n):\n",
    "    res = 0\n",
    "    for i in range(n+1):\n",
    "        res += i*i\n",
    "        sleep(1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's use the function to calulate some sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 373 µs, sys: 849 µs, total: 1.22 ms\n",
      "Wall time: 4.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum_of_squares(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function took `4secs` to compute. Now, if you are as impatient as me...you would have liked to see what's happening while the result is being computed. Some kind of progress report ? Rather than bloating our core function with progress logic, we will write our progress logic in a different function and then pass that function to `sum_of_squares`. It can callback the progress function after every loop. We will have to edit our `sum_of_squares` function with a generic function call after the loop, so that it call any user passed function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(n, cb=None):\n",
    "    res = 0\n",
    "    for i in range(n+1):\n",
    "        res += i*i\n",
    "        sleep(1)\n",
    "        if cb: cb(i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write are `show_progress` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(i):\n",
    "    print(f\"Awesome !!!, We have finished Epoch {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call our sum of squares function with `show_progress` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awesome !!!, We have finished Epoch 0\n",
      "Awesome !!!, We have finished Epoch 1\n",
      "Awesome !!!, We have finished Epoch 2\n",
      "Awesome !!!, We have finished Epoch 3\n",
      "CPU times: user 4.04 ms, sys: 1.84 ms, total: 5.88 ms\n",
      "Wall time: 4.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum_of_squares(3, show_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, our function caluclated the same thing but this time we could see the progress it did also. Now, we will make our function and the callback `show_progress` more flexible. Next, let's add the functionality so that a user can define what `exclamation` they wish to define rather that our hardcoded `Awesome`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(exclamation, epoch):\n",
    "    print(f\"{exclamation} !!!, We have finished Epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `sum_of_squares` expects a callback that takes only one input where as our updated `show_progress` takes two. We will use `partials` to convert our function that takes two arguments into a function that take one argument. Partials allow us to partially apply a function. We can do this because functions are first order things in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_show_progress(exclamation):\n",
    "    def _inner(epoch):  print(f\"{exclamation} !!!, We have finished Epoch {epoch}\")\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice! !!!, We have finished Epoch 0\n",
      "Nice! !!!, We have finished Epoch 1\n",
      "Nice! !!!, We have finished Epoch 2\n",
      "Nice! !!!, We have finished Epoch 3\n",
      "Nice! !!!, We have finished Epoch 4\n",
      "CPU times: user 4.86 ms, sys: 2.04 ms, total: 6.9 ms\n",
      "Wall time: 5.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum_of_squares(4, make_show_progress(\"Nice!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice !!!, Let's try it with a different exclamation token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrific! !!!, We have finished Epoch 0\n",
      "Terrific! !!!, We have finished Epoch 1\n",
      "Terrific! !!!, We have finished Epoch 2\n",
      "Terrific! !!!, We have finished Epoch 3\n",
      "Terrific! !!!, We have finished Epoch 4\n",
      "CPU times: user 4.84 ms, sys: 1.92 ms, total: 6.77 ms\n",
      "Wall time: 5.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum_of_squares(4, make_show_progress(\"Terrific!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrific !!!. It's kind of annoying that every time we call our function with the callback, we loose our stat in callback. Wouldn't it be nice to store things like how long our function took to complete. Let's convert our our callback function into a callback class so that we retain the state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressShowingCallback():\n",
    "    def __init__(self, exclamation=\"Awesome\"): self.exclamation = exclamation\n",
    "    def __call__(self, epoch): print(f\"{self.exclamation}! We've finished epoch {epoch}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = ProgressShowingCallback(\"Terrific\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrific! We've finished epoch 0!\n",
      "Terrific! We've finished epoch 1!\n",
      "Terrific! We've finished epoch 2!\n",
      "Terrific! We've finished epoch 3!\n",
      "Terrific! We've finished epoch 4!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_of_squares(4, cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have introduced a new concept here. That `__call__` thingy. Python has some dunder functions *(yes, that's how we pronounce them)* that get called at some point in the objects lifecycle. `__call__` gets called when we use a object instance of that class as a function, like we did while calling `sum_of_squares` with `cb`. Adimitedly we could have defined a normal function like `callback` and then passed `cb.callback` but doesn't our approach makes things cleaner ? That's the only reason we defined our behaviour on `__call__`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can probably notice by now: we have been able to modify behaviour of our `sum_of_squares` funtion by writing a completely seprate class and hence not bloating the calculation logic.\n",
    "\n",
    "> This is what `Callbacks` allow us to do. `Modify the behaviour` of some underlying function by `writing seprate concise classes` hence keeping the underlying function concise, readable and error prone.\n",
    "\n",
    "Now, you might be thingking hang on, where did we modify the behaviour of the function. Weren't we just priting stuff ? Yes, you are write. So let's fix that and also modify the behaviour of the `sum_of_squares` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap our caculation inside a class called `ShowCalulator`. This will allow us to inject behaviour at multiple places during the computation with same callback. We initialize it with upto what `number we want sum of squares` and `optionally our callback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlowCalculator():\n",
    "    def __init__(self, n, cb=None): self.n,self.cb,self.res = n,cb,0\n",
    "    \n",
    "    def __call__(self, cb_name, *args):\n",
    "        if not self.cb: return\n",
    "        cb = getattr(self.cb,cb_name, None)\n",
    "        if cb: return cb(self, *args)\n",
    "\n",
    "    def calc(self):\n",
    "        for i in range(self.n+1):\n",
    "            self('before_calc', i)\n",
    "            self.res += i*i\n",
    "            sleep(1)\n",
    "            if self('after_calc', i):\n",
    "                print(\"stopping early\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `calc` function now has two `callback points`, called `before_calc` and `after_calc`. If the `after_calc` callback returns true, we will break and stop the computation *(modifying behaviour)*. Our ``__call__`` function now check with `getattr` if callabck has the function, if it has it calls that function. That's how we are able to get multiple callback points with one single callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create our `Modifying Callback` which will have to points `before_calc` which will print result when computation is about to start and `after_calc` which will print result and when the result gets larger than 10 it will stop the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifyingCallback():\n",
    "    \n",
    "    def before_calc(self, calc, i):\n",
    "        print(f\"Starting Epoch {i}: res: {calc.res}\")\n",
    "        \n",
    "    def after_calc (self, calc, epoch):\n",
    "        print(f\"After {epoch}: {calc.res}\")\n",
    "        if calc.res>10: return True\n",
    "        if calc.res<3: calc.res = calc.res*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator = SlowCalculator(5, ModifyingCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0: res: 0\n",
      "After 0: 0\n",
      "Starting Epoch 1: res: 0\n",
      "After 1: 1\n",
      "Starting Epoch 2: res: 2\n",
      "After 2: 6\n",
      "Starting Epoch 3: res: 6\n",
      "After 3: 15\n",
      "stopping early\n"
     ]
    }
   ],
   "source": [
    "calculator.calc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculator.res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have implemented is a general purpose callback system that can be used to inject behaviour in the underlying class `(ShowCalculator)` at any number of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, in typical training loop of a deep learning system performs a set of predifined steps `(get a batch of data, pass it through the model, calculate loss function, calculate gradients using backprop, update parameters and zero gradients)`. So, we will use this concept to write a flexible training loop so that we can inject behaviour at any of those  of those predefined steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python's @property tutorial](https://www.programiz.com/python-programming/property)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first write a base Callback class, that checks through `__call__`, if the function is there and if its present, it calls it. We will also change the name of callback from camel case to snake format. Let's not worry too much about `_order`, `set_runner` and `_getattr` at this moment. We will comeback to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Callback():\n",
    "    _order=0\n",
    "    def set_runner(self, run): self.run=run\n",
    "    def __getattr__(self, k): return getattr(self.run, k)\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        return camel2snake(name or 'callback')\n",
    "    \n",
    "    def __call__(self, cb_name):\n",
    "        f = getattr(self, cb_name, None)\n",
    "        if f and f(): return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually when we are training a model, we train it for a epoch and then evaluate it on the validation set. During validation we turn the model into evaluation format so that we do not update weights. Let's write our first callback to put model in training at the begining of epoch: `begin_epoch` and in validation: `begin_validate` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TrainEvalCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.run.n_epochs=0.\n",
    "        self.run.n_iter=0\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.run.n_epochs += 1./self.iters\n",
    "        self.run.n_iter   += 1\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.run.n_epochs=self.epoch\n",
    "        self.model.train()\n",
    "        self.run.in_train=True\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.run.in_train=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might have to cancel the training either after `training`, after an `Epoch` or `Batch`. Let's define empty exception classes for each so that we can exit neatly at those points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CancelTrainException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelBatchException(Exception): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we defined a `name` function in our base Callback class. This allows us to change the name of the class name to snake format and remove the training callback string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_eval'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainEvalCallback().name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a utility function called `listify` which will turn a set of objects into a python `list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import *\n",
    "\n",
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://nbviewer.jupyter.org/github/fastai/fastai_docs/blob/master/dev_course/dl2/09b_learner.ipynb\n",
    "- https://nbviewer.jupyter.org/github/fastai/fastai_docs/blob/master/dev_course/dl2/04_callbacks.ipynb\n",
    "- https://www.autodeskresearch.com/publications/samestats\n",
    "- https://forums.fast.ai/uploads/default/original/3X/a/3/a3eecbbffd66da600b7ad0b2ad4250c8ed1a682c.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep learning model has lot of surrounding pieces associated with, namely the `model`, `optimizer`, `loss_func` and `databunch`. Let's write a storage class called `Learner` which just stores all this which will make it easy for us to access these when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Learner():\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runner: Writing a flexible training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define our main class called `Runner`. It will be responsible for `registering callback`, `calling callbacks` at appropriate time and `carrying out the training of our model`. It has a lot of code, so we will discuss each function one by one.\n",
    "\n",
    "- `Runner.__init__`: takes `cbs` a.k.a. *callbacks* and `cb_funcs` a.k.a *partially applied callback functions*.\n",
    "    - It first turns the `cbs` into a list if they are not. \n",
    "    - Goes through each `cb_funcs`, calls it, sets the name atrribute to the callback use name function and appends to the list of callback\n",
    "    - Registers a `stop` valiable and initializes to `False`.\n",
    "    - Registers a `cbs variable with a list of callbacks ` that has TrainEvalCallback and cbs.\n",
    "    \n",
    "    \n",
    " \n",
    "- `Runner.opt`: An accessor to `optimizer` inside the learner object.\n",
    "- `Runner.model`: An accessor to `model` inside the learner object.\n",
    "- `Runner.loss_func`: An accessor to `loss_func` inside the learner object.\n",
    "- `Runner.data`: An accessor to `data` inside the learner object.\n",
    "\n",
    "\n",
    "- `Runner.__call__`\n",
    "    - Goes through all the registered callbacks in `self.cbs` in `_order`, and calls the callback and returns if the callback returns something.\n",
    "    \n",
    "    \n",
    "- `Runner.all_batches`:\n",
    "    - Registers a `iters` variable.\n",
    "    - Loops through all the mini-batches in the dataloader and calls `one_batch`.\n",
    "    \n",
    "    \n",
    "- `Runner.one_batch`:\n",
    "    - It gets one batch of `x's` and `y's` and registers to the variable. \n",
    "    - Calls `begin_batch` callback functions for all registered callbacks. \n",
    "    - Passes the `xb` into the model and registers the result into `pred`. \n",
    "    - Calls `after_pred` callback functions for all registered callbacks. \n",
    "    - Calculates the loss between `pred` and `ys` using `loss_func` and register to the class.\n",
    "    - Calls `after_loss` callback functions for all registered callbacks. \n",
    "    - Returns if the model is in `validation` mode, checked through `not self.in_train`.\n",
    "    - Calls `after_loss` callback functions for all registered callbacks. \n",
    "    - Calculates the gradient with respect to the weights by calling `loss.backward()`.\n",
    "    - Updates the parameters in the optimizer by calling `opt.step()`\n",
    "    - Calls `after_step` callback functions for all registered callbacks. \n",
    "    - Zeros the gradients so there is accumlation for the next batch.\n",
    "    - If the callbacks have a `CancelBatchException` then breaks calls `after_cancel_batch` callback functions for all registered callbacks.\n",
    "    - And finally, calls `after_batch` callback functions for all registered callbacks.\n",
    "    \n",
    "- `Runner.fit`: \n",
    "    - Recieves, number of `epochs` to train for, the `learn` object.\n",
    "    - Registers `epoch` and `learn`. Also initializes the `loss` to `0`. \n",
    "    - Tries:\n",
    "        - Sets the runner object inside all the callbacks by looping through all of them and calling `set_runner` function. That's what `set_runner does that we defined in our base Callback function.\n",
    "        - Calls `begin_fit` callback functions for all registered callbacks.\n",
    "        - Iterates for epoch times, doing:\n",
    "            - Calls `begin_epoch` callback functions for all registered callbacks.\n",
    "            - If none of them break: goes on to call `all_batches` with the training data.\n",
    "            - Then inside a with block to avoid calculating grad, Calls `begin_validate` callback functions for all registered callbacks. If none of them break then, \n",
    "            - Calls the `all_batches` on validation dataloader. \n",
    "            - Then, Calls `after_fit` callback functions for all registered callbacks.\n",
    "    - If there is  `CancelTrainException`, then stops the training and calls `after_cancel_train` function for all the registered callbacks. \n",
    "    - Finally, calls `after_fit` function for all the registered callbacks.\n",
    "    - And sets the `learn` object to none. So we dont accedently call fit again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That completes our, possibly the longest class. It's ok if you didn't understand everything. Rewatch the `fast.ai` lecture again, read this again untill you understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        cbs = listify(cbs)\n",
    "        for cbf in listify(cb_funcs):\n",
    "            cb = cbf()\n",
    "            setattr(self, cb.name, cb)\n",
    "            cbs.append(cb)\n",
    "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
    "\n",
    "    @property\n",
    "    def opt(self):       return self.learn.opt\n",
    "    @property\n",
    "    def model(self):     return self.learn.model\n",
    "    @property\n",
    "    def loss_func(self): return self.learn.loss_func\n",
    "    @property\n",
    "    def data(self):      return self.learn.data\n",
    "\n",
    "    def one_batch(self, xb, yb):\n",
    "        try:\n",
    "            self.xb,self.yb = xb,yb\n",
    "            self('begin_batch')\n",
    "            self.pred = self.model(self.xb)\n",
    "            self('after_pred')\n",
    "            self.loss = self.loss_func(self.pred, self.yb)\n",
    "            self('after_loss')\n",
    "            if not self.in_train: return\n",
    "            self.loss.backward()\n",
    "            self('after_backward')\n",
    "            self.opt.step()\n",
    "            self('after_step')\n",
    "            self.opt.zero_grad()\n",
    "        except CancelBatchException: self('after_cancel_batch')\n",
    "        finally: self('after_batch')\n",
    "\n",
    "    def all_batches(self, dl):\n",
    "        self.iters = len(dl)\n",
    "        try:\n",
    "            for xb,yb in progress_bar(dl, parent=self.mbar): self.one_batch(xb, yb)\n",
    "        except CancelEpochException: self('after_cancel_epoch')\n",
    "\n",
    "    def fit(self, epochs, learn):\n",
    "        self.epochs,self.learn,self.loss = epochs,learn,tensor(0.)\n",
    "\n",
    "        try:\n",
    "            for cb in self.cbs: cb.set_runner(self)\n",
    "            self.mbar = master_bar(range(epochs))\n",
    "            self('begin_fit')\n",
    "            for epoch in self.mbar:\n",
    "                self.epoch = epoch\n",
    "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
    "\n",
    "                with torch.no_grad(): \n",
    "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
    "                self('after_epoch')\n",
    "            \n",
    "        except CancelTrainException: self('after_cancel_train')\n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.learn = None\n",
    "\n",
    "    def __call__(self, cb_name):\n",
    "        res = False\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have written our training loop, we can train the our models. So let's train one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model(data)\n",
    "learn = Learner(model, opt, loss_func, data)\n",
    "run = Runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:03 <p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.fit(5,learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh, what just happened ? We got back nothing. That's because we never printed or told our loop to report back stuff. But our `run` object has all the state we will want, we just need to intercept them at appropriate time during training and format them in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " False,\n",
       " 10,\n",
       " [<__main__.TrainEvalCallback at 0x1a26f85208>],\n",
       " tensor(0.2906),\n",
       " torch.Size([784, 10]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.epochs, run.in_train, run.iters, run.cbs, run.loss, run.pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But aren't we supposed to track what happens after very epoch ? Yes. So, lets write a `callback` to report back to us some `metrics`. In particular we would like to see `train_loss`, `valid_loss`, `train_accuracy` and `valid_accuracy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we need two sets of metrics one for train data and one for valid data. So lets write a class that will print `average statistics for each of the data loaders`. We wil call it surpise `AvgStats`. The only functions worth remembering are:\n",
    "\n",
    "- `AvgStats.accumulate`:\n",
    "    - It has the accesse to runner object, that means it has acces to every state in the learner object. \n",
    "    - first we figure out number of elements in the batch and assign it `bn`.\n",
    "    - we multiply the loss by `bn` and add it to `tot_loss` (total_loss). \n",
    "    - we add `bn` to `cout` (that keeps track of total datapoints seen during training).\n",
    "    - Then we caculate each metric by calling it with the predictions and labels and then multiply it by `bn`. \n",
    "\n",
    "\n",
    "- `AvgStats.avg_stats`: averages the metrics with total count of element seen during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgStats():\n",
    "    def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train\n",
    "    \n",
    "    def reset(self):\n",
    "        self.tot_loss,self.count = 0.,0\n",
    "        self.tot_mets = [0.] * len(self.metrics)\n",
    "        \n",
    "    @property\n",
    "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
    "    @property\n",
    "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if not self.count: return \"\"\n",
    "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
    "\n",
    "    def accumulate(self, run):\n",
    "        bn = run.xb.shape[0]\n",
    "        self.tot_loss += run.loss * bn\n",
    "        self.count += bn\n",
    "        for i,m in enumerate(self.metrics):\n",
    "            self.tot_mets[i] += m(run.pred, run.yb) * bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our function calculate average statistics, we can write our callback that will help us to intercept metrics as our model trains and print them at the end of every epoch. We will call our callback `AvgStatsCallback`.\n",
    "\n",
    "- `AvgStatsCallback.__init__`:\n",
    "    - registers and initializes `AvgStats` for both train data and valid data. Note: since `AvgStats` takes a list of metrics and `in_train` variable we pass `True` for training and `False` for validation set.\n",
    "    \n",
    "- `AvgStatsCallback.begin_epoch`:\n",
    "    - resets the metric classes for both set of data and starts the timer.\n",
    "    \n",
    "- `AvgStatsCallback.after_loss`:\n",
    "    - After loss has been calculated in our training loop, it accumulates the stats for the data. Note we do that in with no grad block because we do not want to affect our graidients as `backward` hasn't been called yet. \n",
    "    \n",
    "- `AvgStatsCallback.after_epoch`:\n",
    "    - We prepare the stats to be printed.\n",
    "    - Also calculate the time taken for whole epoch. \n",
    "    - We finally print the stats for the last epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
    "    \n",
    "    def begin_fit(self):\n",
    "        met_names = ['loss'] + [m.__name__ for m in self.train_stats.metrics]\n",
    "        names = ['epoch'] + [f'train_{n}' for n in met_names] + [f'valid_{n}' for n in met_names] + ['time']\n",
    "        self.mbar.write(names, table=True)\n",
    "    \n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.run)\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        stats = [str(self.epoch)] \n",
    "        for o in [self.train_stats, self.valid_stats]: stats += [f'{v:.6f}' for v in o.avg_stats] \n",
    "        stats += [format_time(time.time() - self.start_time)]\n",
    "        self.mbar.write(stats, table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our metric reporting callback, let's use it to see some metrics while we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model(data)\n",
    "learn = Learner(model, opt, loss_func, data)\n",
    "# we call it using partial beause it takes the name of metric we want to calculate\n",
    "cb_funcs = [partial(AvgStatsCallback, accuracy)] \n",
    "run = Runner(cb_funcs=cb_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:05 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.283470</td>\n",
       "      <td>0.697320</td>\n",
       "      <td>0.585310</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.507945</td>\n",
       "      <td>0.870760</td>\n",
       "      <td>0.396331</td>\n",
       "      <td>0.895300</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.400549</td>\n",
       "      <td>0.890580</td>\n",
       "      <td>0.345599</td>\n",
       "      <td>0.903600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.357919</td>\n",
       "      <td>0.900140</td>\n",
       "      <td>0.316326</td>\n",
       "      <td>0.912100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.332844</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.298359</td>\n",
       "      <td>0.916900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.314739</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>0.284019</td>\n",
       "      <td>0.919900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.300308</td>\n",
       "      <td>0.913820</td>\n",
       "      <td>0.274106</td>\n",
       "      <td>0.920700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.fit(7, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Voila, we get the metric we asked for for each epoch. Note we also get the total training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting hyperparams and losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice if we can see how our loss varies during training in a plot and also see how our hyper-params change during training. We can do this with callbacks :). Let's write another callback, that records the losses during training and also the learning rate so that we can plot them later.\n",
    "\n",
    "- `Recorder.after_batch`:\n",
    "    - Appends the training loss to a list `self.losses`.\n",
    "    - Also appends the `learning rate` used for that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.lrs = [[] for _ in self.opt.param_groups]\n",
    "        self.losses = []\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())        \n",
    "\n",
    "    def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])\n",
    "    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])\n",
    "        \n",
    "    def plot(self, skip_last=0, pgid=-1):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        lrs    = self.lrs[pgid]\n",
    "        n = len(losses)-skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(lrs[:n], losses[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our new `Recorder` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model(data)\n",
    "learn = Learner(model, opt, loss_func, data)\n",
    "cb_funcs = [Recorder, partial(AvgStatsCallback, accuracy)]\n",
    "run = Runner(cb_funcs=cb_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:05 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.252160</td>\n",
       "      <td>0.699040</td>\n",
       "      <td>0.567708</td>\n",
       "      <td>0.874800</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.500153</td>\n",
       "      <td>0.872720</td>\n",
       "      <td>0.391054</td>\n",
       "      <td>0.898400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.399960</td>\n",
       "      <td>0.890940</td>\n",
       "      <td>0.341384</td>\n",
       "      <td>0.904500</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.360120</td>\n",
       "      <td>0.899060</td>\n",
       "      <td>0.319764</td>\n",
       "      <td>0.911900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.336599</td>\n",
       "      <td>0.905020</td>\n",
       "      <td>0.300791</td>\n",
       "      <td>0.914700</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.319790</td>\n",
       "      <td>0.909300</td>\n",
       "      <td>0.288114</td>\n",
       "      <td>0.917900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.306096</td>\n",
       "      <td>0.913560</td>\n",
       "      <td>0.278464</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.fit(7, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can access the recorder callback in run to plot our `loss` and `learning rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the learning rate didn't change during training, because we never told it to. Probably it is a good idea to have some kind of schedule for our learning rate. It's a great idea, which we will implement in the next post *(Link to scheduler post: Coming soon)*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we started with raw pytorch and wrote a bunch of convienient classes to wrap our data `DataBunch`, Learnt about callbacks in Python, Wrote a very flexible training loop, implemented couple of callbacks to report metrics and plot losses. We will build on these classes as we continue studying more topics in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you made it till here, well done !!!. We have made our life super easy going forward. This investment will allow us to focus on other fun stuff such as designing better architectures, studying better optimizers, better scheduling of our hyperparameters and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: This post `heavily borrows (almost everything)` from first 2 lectures of [fast.ai](https://www.fast.ai/) part2 2019. This course hasn't been officially released. Tentative release date June 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export as Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb to fastlib/nb_00.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 00_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-fastai",
   "language": "python",
   "name": "my-fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
